{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebfd192b",
   "metadata": {},
   "source": [
    "# Brewtiful: A Streamlit-Powered Assistant\n",
    "This notebook contains the code for a Streamlit-based chatbot that interacts with markdown notes. The assistant is designed to help users manage and retrieve information from their notes efficiently. \n",
    "\n",
    "## Key Features\n",
    "\n",
    "### LangChain\n",
    "The application utilizes the **LangChain** library, a framework designed to simplify the development of applications powered by large language models (LLMs). LangChain provides modular components and tools that can be chained together to create sophisticated workflows. Its key benefits include:\n",
    "- **Modularity**: Offers reusable components for common LLM tasks (e.g., interacting with models, managing prompts, retrieving data).\n",
    "- **Composition**: Allows developers to easily combine these components into \"chains\" or more complex \"agents\" to build custom applications.\n",
    "- **Integration**: Provides integrations with various LLM providers, data sources, and tools.\n",
    "\n",
    "\n",
    "The application leverages NLP (natural language processing) techniques facilitated by LangChain to provide meaningful insights and answers to user queries:\n",
    "- **Text Preprocessing**: Cleans and structures the raw text data for further analysis.\n",
    "- **Embeddings**: Converts text into numerical vectors using models integrated via LangChain to capture semantic meaning, enabling similarity searches and document retrieval.\n",
    "- **Text Splitting**: Breaks down large documents into smaller, manageable chunks using LangChain's text splitters to ensure efficient processing and context preservation.\n",
    "- **Context-Aware Responses**: Uses LangChain's memory modules to maintain the flow of conversation, ensuring that responses are relevant to the user's query and the chat history.\n",
    "- **Prompt Engineering**: Defines the behavior of the assistant through carefully crafted instructions using LangChain's prompt templates, ensuring accurate and helpful responses.\n",
    "\n",
    "### Streamlit for User Interface\n",
    "Streamlit is a powerful framework for building interactive web applications in Python. It allows developers to create user-friendly interfaces with minimal effort, making it an good choice for deploying machine learning and data science applications. In this project, Streamlit is used to create a simple yet effective interface for users to interact with the chatbot.\n",
    "\n",
    "## Application Workflow Summary\n",
    "The application follows these main steps, mirroring the structure of the code cells:\n",
    "\n",
    "### 1. Set-Up\n",
    "- 1.1.  **Import Required Libraries**: Load necessary Python packages from standard, third-party, and LangChain libraries.\n",
    "- 1.2.  **Load Environment Variables**: Access the `GOOGLE_API_KEY` from a `.env` file.\n",
    "\n",
    "### 2. Definition of Functions and Variables\n",
    "- 2.1.  **Initialize Session State**: Define and run `initialize_session_state` to ensure variables persist across Streamlit reruns.\n",
    "- 2.2.  **Define Data Loading and Preprocessing**: Create the `load_and_preprocess_notes` function to read markdown files, extract metadata, and handle errors.\n",
    "- 2.3.  **Define Text Splitting**: Create the `split_documents` function using `RecursiveCharacterTextSplitter` to break notes into chunks.\n",
    "- 2.4.  **Define Document Filtering**: Create the `filter_documents` function to allow selection based on metadata (title, categories, tags).\n",
    "- 2.5.  **Define Document Formatting**: Create the `format_docs` function to prepare retrieved document content for display or input to the LLM.\n",
    "- 2.6.  **Define Chat History Retrieval**: Create the `get_chat_history` function to extract conversation history from memory.\n",
    "- 2.7.  **Define LangChain Chain Creation**: Create the `create_chain` function, which assembles the prompt template, retriever, memory access, and LLM into a runnable sequence.\n",
    "\n",
    "### 3. Main Application Logic\n",
    "\n",
    "- 3.1. **Define Main Application Logic (`main` function)**: This function orchestrates the application:\n",
    "    - Sets the Streamlit page title and header.\n",
    "    - Calls `initialize_session_state`.\n",
    "    - Performs one-time loading/processing of notes, embeddings, vector store creation, LLM setup, and memory initialization, storing results in session state.\n",
    "    - Sets up the Streamlit sidebar UI for filtering.\n",
    "    - Sets up preloaded prompt buttons.\n",
    "    - Displays the chat history.\n",
    "    - Handles user input via `st.chat_input`.\n",
    "    - If a query exists: displays it, filters documents, sets up the retriever, invokes the chain (if documents exist), handles the response (display, memory update), and resets button state.\n",
    "\n",
    "- 3.2. **Run Application**: \n",
    "    - The `if __name__ == \"__main__\":` block calls the `main` function to start the Streamlit app when the script is executed directly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4c6db8",
   "metadata": {},
   "source": [
    "## 1. Set-Up\n",
    "\n",
    "### 1.1 Import Required Libraries\n",
    "\n",
    "This section imports the necessary libraries. These include:\n",
    "- **Standard libraries** like `os`, `pathlib`, and `yaml` for file, path management, and YAML parsing.\n",
    "- **Third-party libraries** like `streamlit` for building the web app, and `dotenv` for loading environment variables.\n",
    "- **LangChain components**: Specific modules from the LangChain framework are imported to handle core LLM-related tasks. This includes interfaces for chat models (`ChatGoogleGenerativeAI`), embedding models (`HuggingFaceEmbeddings`), vector stores (`FAISS`), prompt templating (`ChatPromptTemplate`), runnable components (`RunnableLambda`), document representation (`Document`), text splitting (`RecursiveCharacterTextSplitter`), and conversation memory (`ConversationBufferMemory`).\n",
    "These libraries are essential for the functionality of the application, enabling tasks like file handling, environment configuration, and the complex NLP processing managed by LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6053fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Library Imports ---\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "# --- Third-Party Imports ---\n",
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- LangChain imports ---\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25f6ef1",
   "metadata": {},
   "source": [
    "### 1.2 Load Environment Variables\n",
    "Environment variables are used to securely store sensitive information like API keys. This step loads these variables from a `.env` file using the `dotenv` library. For example, the `GOOGLE_API_KEY` is required to access Google's Generative AI services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7af5cfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Environment Variables ---\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988c1d15",
   "metadata": {},
   "source": [
    "## 2. Definition of Functions and Variables\n",
    "\n",
    "### 2.1 Initialize Session State\n",
    "Streamlit uses a session state to persist data across user interactions. This function initializes variables to ensure they are available throughout the app. Without this step, the app would not be able to maintain state between user actions. \n",
    "\n",
    "The session state is a dictionary-like object (`st.session_state`) that stores data specific to a user's current browser session. It's crucial because Streamlit reruns the entire script on each interaction; session state allows variables (like chat history, loaded data, or model instances) to persist across these reruns, maintaining the application's context. As the user interacts with the chatbot (e.g., sending a message, applying filters), the relevant data (like the `messages` list or `user_query`) is updated within `st.session_state` by the application logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9a3d43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "def initialize_session_state():\n",
    "    \"\"\"Initialize session state variables if they do not exist.\"\"\"\n",
    "    if \"all_notes\" not in st.session_state:\n",
    "        st.session_state.all_notes = []\n",
    "    if \"documents\" not in st.session_state:\n",
    "        st.session_state.documents = []\n",
    "    if \"embeddings\" not in st.session_state:\n",
    "        st.session_state.embeddings = None\n",
    "    if \"db\" not in st.session_state:\n",
    "        st.session_state.db = None\n",
    "    if \"llm\" not in st.session_state:\n",
    "        st.session_state.llm = None\n",
    "    if \"memory\" not in st.session_state:\n",
    "        st.session_state.memory = None\n",
    "    if \"user_query\" not in st.session_state:\n",
    "        st.session_state.user_query = None\n",
    "    if \"messages\" not in st.session_state:\n",
    "        st.session_state.messages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a331bb7b",
   "metadata": {},
   "source": [
    "### 2.2 Load and Preprocess Notes\n",
    "Preprocessing involves reading markdown files, extracting their content and metadata, and preparing them for further analysis. The `yaml` library is used to parse the metadata (e.g., title, tags, categories) embedded in the markdown files' front matter. This step ensures that the notes are structured and ready for NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b93f1034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Loading and Preprocessing ---\n",
    "def load_and_preprocess_notes(notes_dir=\"Notes\"):\n",
    "    \"\"\"\n",
    "    Load and preprocess markdown notes from the specified directory.\n",
    "\n",
    "    Args:\n",
    "        notes_dir (str): Path to the directory containing markdown files.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples containing filename, content, and metadata.\n",
    "    \"\"\"\n",
    "    all_notes = []\n",
    "    notes_path = Path(notes_dir)\n",
    "\n",
    "    if not notes_path.exists() or not notes_path.is_dir():\n",
    "        st.error(f\"Error: '{notes_dir}' is not a valid directory.\")\n",
    "        return []\n",
    "\n",
    "    markdown_files = list(notes_path.rglob(\"*.md\"))\n",
    "\n",
    "    if not markdown_files:\n",
    "        st.warning(f\"No markdown files found in '{notes_dir}'.\")\n",
    "        return []\n",
    "\n",
    "    for file_path in markdown_files:\n",
    "        try:\n",
    "            # Skip README.md\n",
    "            if file_path.name.lower() == \"readme.md\":\n",
    "                continue\n",
    "\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "\n",
    "            # Extract YAML front matter\n",
    "            if content.startswith(\"---\"):\n",
    "                end_index = content.find(\"---\", 3)\n",
    "                if end_index != -1:\n",
    "                    yaml_content = content[3:end_index].strip()\n",
    "                    metadata = yaml.safe_load(yaml_content) if yaml_content else {}\n",
    "                    content = content[end_index + 3:].strip()\n",
    "                else:\n",
    "                    metadata = {} # No closing '---' found\n",
    "            else:\n",
    "                metadata = {} # No front matter detected\n",
    "\n",
    "            all_notes.append((file_path.name, content, metadata))\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error reading or processing {file_path}: {e}\")\n",
    "    return all_notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d00952",
   "metadata": {},
   "source": [
    "### 2.3 Split Documents\n",
    "Markdown notes are often lengthy, making it difficult to process them as a whole. This function splits the notes into smaller chunks using a text splitter. The `chunk_size` defines the maximum size of each chunk, while `chunk_overlap` ensures some overlap between chunks to preserve context. This step is crucial for NLP tasks like embeddings and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a85b849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Text Splitting ---\n",
    "def split_documents(notes_data, chunk_size=1000, chunk_overlap=50):\n",
    "    \"\"\"\n",
    "    Split notes into smaller chunks for processing.\n",
    "\n",
    "    Args:\n",
    "        notes_data (list): List of tuples containing filename, content, and metadata.\n",
    "        chunk_size (int): Maximum size of each chunk.\n",
    "        chunk_overlap (int): Overlap between chunks.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Document objects.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    docs = []\n",
    "    for filename, content, metadata in notes_data:\n",
    "        split_texts = text_splitter.split_text(content)\n",
    "        for text in split_texts:\n",
    "            doc_metadata = {\"source\": filename, **metadata}\n",
    "            docs.append(Document(page_content=text, metadata=doc_metadata))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b7ae50",
   "metadata": {},
   "source": [
    "### 2.4 Filter Documents\n",
    "Filtering allows users to narrow down the documents based on specific criteria like title, categories, or tags. This function iterates through the documents and applies the filters provided by the user. It checks if the selected categories/tags are a subset of the document's categories/tags. This step ensures that only relevant documents are retrieved for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fff2cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Filter Documents ---\n",
    "def filter_documents(documents, selected_title=None, selected_categories=None, selected_tags=None):\n",
    "    \"\"\"\n",
    "    Filter documents based on title, categories, and tags.\n",
    "\n",
    "    Args:\n",
    "        documents (list): List of Document objects.\n",
    "        selected_title (str): Title to filter by.\n",
    "        selected_categories (list): Categories to filter by.\n",
    "        selected_tags (list): Tags to filter by.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of filtered Document objects.\n",
    "    \"\"\"\n",
    "    filtered_docs = documents\n",
    "\n",
    "    if selected_title:\n",
    "        filtered_docs = [doc for doc in filtered_docs if doc.metadata.get('title') == selected_title]\n",
    "\n",
    "    if selected_categories:\n",
    "        # Check if selected categories are a subset of document categories\n",
    "        filtered_docs = [\n",
    "            doc for doc in filtered_docs\n",
    "            if doc.metadata.get('categories') is not None and set(selected_categories).issubset(set(doc.metadata.get('categories')))\n",
    "        ]\n",
    "\n",
    "    if selected_tags:\n",
    "        # Check if selected tags are a subset of document tags\n",
    "        filtered_docs = [\n",
    "            doc for doc in filtered_docs\n",
    "            if doc.metadata.get('tags') is not None and set(selected_tags).issubset(set(doc.metadata.get('tags')))\n",
    "        ]\n",
    "\n",
    "    return filtered_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb11d67",
   "metadata": {},
   "source": [
    "### 2.5 Format Documents\n",
    "Formatting prepares the documents for display in the Streamlit app. It creates a readable string representation of the document content and metadata, making it easier for users to understand the retrieved information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a6389bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Format Docs ---\n",
    "def format_docs(docs):\n",
    "    \"\"\"\n",
    "    Format documents for display.\n",
    "\n",
    "    Args:\n",
    "        docs (list): List of Document objects.\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted string representation of documents.\n",
    "    \"\"\"\n",
    "    return \"\\n\\n\".join(f\"Source: {doc.metadata['source']}\\nContent: {doc.page_content}\" for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3a702e",
   "metadata": {},
   "source": [
    "### 2.6 Retrieve Chat History\n",
    "The chat history is stored in memory to maintain context across user interactions. This function retrieves and formats the chat history, which is essential for generating context-aware responses.\n",
    "\n",
    "**Note on memory duration**\n",
    "In this application, the memory is stored as part of the Streamlit session state (`st.session_state`). This means:\n",
    "- The memory persists only for the duration of the user's session in the browser.\n",
    "- Once the session ends (e.g., the browser is closed or refreshed), the memory is cleared.\n",
    "- The memory is not stored persistently across sessions or written to disk in this implementation. If persistent memory is required, additional mechanisms like database storage would need to be implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65f366c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Chat History ---\n",
    "def get_chat_history(memory):\n",
    "    \"\"\"\n",
    "    Retrieve chat history from memory.\n",
    "\n",
    "    Args:\n",
    "        memory (ConversationBufferMemory): Memory object containing chat history.\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted chat history.\n",
    "    \"\"\"\n",
    "    messages = memory.chat_memory.messages\n",
    "    return \"\\n\".join([f\"{m.type}: {m.content}\" for m in messages])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80a1cdc",
   "metadata": {},
   "source": [
    "### 2.7 Create LangChain Processing Chain\n",
    "LangChain is a framework for building applications powered by language models. It allows developers to connect different components (like LLMs, data sources, and tools) into cohesive workflows called \"Chains\". This function creates a specific processing chain tailored for this application, combining a language model, a document retriever, and conversational memory.\n",
    "\n",
    "#### Key LangChain Concepts Used Here:\n",
    "- **LangChain Framework**: A library providing standard interfaces and integrations for building LLM applications. It simplifies tasks like prompt management, data connection, model interaction, and state management.\n",
    "- **Chains**: The core concept in LangChain. Chains represent sequences of calls, either to LLMs or other utilities. They allow for structured workflows where the output of one step becomes the input for the next. This application uses a chain to structure the flow from receiving a user question to generating a response based on retrieved context and chat history.\n",
    "- **Components**: LangChain provides various building blocks:\n",
    "    - **LLMs/Chat Models**: Interfaces to interact with language models (e.g., `ChatGoogleGenerativeAI`).\n",
    "    - **Prompts**: Templates for constructing the input to LLMs, often combining user input, context, and instructions (e.g., `ChatPromptTemplate`).\n",
    "    - **Retrievers**: Components that fetch relevant data (like documents from `FAISS`) based on a query.\n",
    "    - **Memory**: Mechanisms to store and recall information from previous interactions in a conversation (e.g., `ConversationBufferMemory`).\n",
    "- **System Prompt**: A predefined instruction within the `ChatPromptTemplate` that sets the overall behavior and persona of the assistant. For example, the assistant is instructed to answer based on the provided context and chat history.\n",
    "- **RunnableLambda**: A flexible component in LangChain's expression language (LCEL) that allows wrapping arbitrary functions into runnable parts of a chain. This enables dynamic data transformations at runtime, such as formatting documents (`format_docs`) or retrieving chat history (`get_chat_history`) within the chain's execution flow.\n",
    "\n",
    "This chain integrates the retriever (for fetching relevant documents), the memory (for maintaining chat history), and the language model (for generating responses), orchestrated by the prompt template. Together, they form a cohesive pipeline for processing user queries contextually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39d083f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create Chain ---\n",
    "def create_chain(llm, retriever, memory):\n",
    "    \"\"\"\n",
    "    Create a LangChain processing chain.\n",
    "\n",
    "    Args:\n",
    "        llm (ChatGoogleGenerativeAI): Language model.\n",
    "        retriever (FAISS): Document retriever.\n",
    "        memory (ConversationBufferMemory): Memory object.\n",
    "\n",
    "    Returns:\n",
    "        dict: LangChain processing chain.\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful assistant. Answer based on the context and the chat history. If you don't know, say so.\\n\\nContext:\\n{context}\\n\\nChat History:\\n{chat_history}\"), # Removed \"Answer only in rhymes.\"\n",
    "        (\"user\", \"{question}\")\n",
    "    ])\n",
    "\n",
    "    return (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(format_docs),\n",
    "            \"question\": RunnableLambda(lambda x: x),\n",
    "            \"chat_history\": RunnableLambda(lambda x: get_chat_history(memory)),\n",
    "        }\n",
    "        | prompt\n",
    "        | (llm if llm else RunnableLambda(lambda x: x)) # Pass through if llm is None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee987e8c",
   "metadata": {},
   "source": [
    "## 3. Main Application Logic\n",
    "\n",
    "### 3.1 Main Function Definition\n",
    "The `main` function orchestrates the Streamlit application. It initializes the necessary components, handles user interactions, manages the application state, and integrates the LangChain elements to process queries and generate responses. Here's a breakdown of the key steps as they are initialized or used within this function:\n",
    "\n",
    "1.  **Session State Initialization**: Ensures that variables like notes, documents, embeddings, vector store, LLM, and memory persist across user interactions within the same session using `st.session_state`. This is crucial for maintaining the context of the application between reruns triggered by user actions.\n",
    "2.  **Data Loading and Processing**: Loads markdown notes from the specified directory, preprocesses them (extracting content and metadata), and splits them into manageable `Document` objects using `load_and_preprocess_notes` and `split_documents`. This step prepares the raw data for embedding and retrieval.\n",
    "3.  **Embeddings Initialization**: If not already in the session state, numerical representations (embeddings) of text are generated using an embedding model (`HuggingFaceEmbeddings` with \"BAAI/bge-small-en-v1.5\"). These vectors capture semantic meaning, allowing for similarity comparisons between the query and the documents.\n",
    "4.  **Vector Store Creation**: If not already in the session state, a `FAISS` vector store (`st.session_state.db`) is created using the generated embeddings and the processed documents. This store indexes the document vectors for efficient similarity searches.\n",
    "5.  **LLM Initialization**: If not already in the session state, the language model (`ChatGoogleGenerativeAI` with \"gemini-2.0-flash-lite\") responsible for understanding the context and generating human-like responses is initialized, requiring the `GOOGLE_API_KEY`.\n",
    "6.  **Memory Initialization**: If not already in the session state, conversational memory (`ConversationBufferMemory`) is set up to store the history of the chat interaction, allowing the LLM to generate contextually relevant responses based on previous turns.\n",
    "7.  **User Interface (Sidebar Filters & Preloaded Prompts)**: Streamlit components are used to create the user interface.\n",
    "    *   The sidebar dynamically populates filter options (title, categories, tags) based on the loaded documents and allows users to select criteria.\n",
    "    *   Preloaded prompt buttons offer users quick ways to ask common questions, setting the `st.session_state.user_query`.\n",
    "8.  **User Interaction (Chat Interface)**: The main chat interface displays the conversation history stored in `st.session_state.messages` and provides an input box (`st.chat_input`) for the user to ask questions, which also sets `st.session_state.user_query`.\n",
    "9.  **Dynamic Filtering and Retrieval**: When `st.session_state.user_query` is set (either via input or button click), the application:\n",
    "    *   Applies any active filters selected in the sidebar to the full document list using `filter_documents`; if no filters are selected, this step effectively uses the complete list of documents.\n",
    "    *   If the resulting `filtered_docs` list is not empty, it creates a ***new, temporary***, filtered `FAISS` vector store (`db_filtered`) for this specific query using only these documents and the existing embeddings. A corresponding `dynamic_retriever` is then created from this temporary store. This ensures the subsequent search only considers documents matching the current filters. *(Note: Creating a new vector store on every query can be resource-intensive (CPU, memory, time) especially with many documents. This approach prioritizes filtering accuracy over performance; optimizations like caching the filtered store or post-retrieval filtering could be considered for larger scale applications.)*\n",
    "    *   If `filtered_docs` is empty (due to filters or no documents loaded), it sets up a dummy retriever that returns nothing and issues the warning `\"No documents match the filters.\"` using `st.warning()`.\n",
    "10. **Chain Creation and Invocation**: If an LLM is available and there are documents to retrieve from (i.e., `filtered_docs` is not empty):\n",
    "    *   The LangChain processing chain is created *dynamically* using `create_chain`, passing the LLM, the `dynamic_retriever` (which might be based on all documents or a filtered subset), and the `memory`.\n",
    "    *   The chain is invoked with the `st.session_state.user_query`. This triggers the sequence: retrieve context -> get chat history -> format prompt -> call LLM.\n",
    "11. **Response Handling**:\n",
    "    *   The generated response content is extracted from the chain's output.\n",
    "    *   Both the user query and the AI response are added to the `st.session_state.memory` and the `st.session_state.messages` list (for display).\n",
    "    *   The response is displayed in the chat interface using `st.chat_message`.\n",
    "    *   Error handling is included for potential issues during chain invocation.\n",
    "    *   Specific handling is added for cases where filters result in no documents, providing a direct message to the user.\n",
    "12. **State Reset**: If the query came from a button click, `st.session_state.user_query` is reset to `None` to prevent re-triggering on the next Streamlit rerun.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8219c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.1 Main Function Definition ---\n",
    "def main():\n",
    "    \"\"\"Main function for the Streamlit app.\"\"\"\n",
    "    st.title(\"Welcome to Brewtiful ☕\")\n",
    "    st.subheader(\"What questions do you have for us?\")\n",
    "\n",
    "    # --- Step 1. Session State Initialization ---\n",
    "    initialize_session_state()\n",
    "\n",
    "    # --- Step 2. Data Loading and Processing ---\n",
    "    if not st.session_state.all_notes:\n",
    "        st.session_state.all_notes = load_and_preprocess_notes()\n",
    "        st.session_state.documents = split_documents(st.session_state.all_notes) if st.session_state.all_notes else []\n",
    "\n",
    "    # --- LangChain Component Setup (Steps 3-6) ---\n",
    "    if st.session_state.documents:\n",
    "    # --- Step 3. Embeddings Initialization ---\n",
    "        if not st.session_state.embeddings:\n",
    "            st.session_state.embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "    # --- Step 4. Vector Store Creation ---\n",
    "        if not st.session_state.db:\n",
    "            st.session_state.db = FAISS.from_documents(st.session_state.documents, st.session_state.embeddings)\n",
    "    # --- Step 5. LLM Initialization ---\n",
    "        if not st.session_state.llm and GOOGLE_API_KEY:\n",
    "            # Updated model name\n",
    "            st.session_state.llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\", google_api_key=GOOGLE_API_KEY, temperature=0.5)\n",
    "        elif not st.session_state.llm:\n",
    "            # Set LLM to None if key is missing, chain will handle this\n",
    "            st.session_state.llm = None\n",
    "    # --- Step 6. Memory Initialization ---\n",
    "        if not st.session_state.memory:\n",
    "            st.session_state.memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True, input_key=\"question\")\n",
    "    else:\n",
    "        st.warning(\"No documents available.\")\n",
    "        return # Exit if no documents to process\n",
    "\n",
    "    # --- Step 7. User Interface Setup(Sidebar Filters & Preloaded Prompts) ---\n",
    "    with st.sidebar:\n",
    "        st.header(\"Filters\")\n",
    "\n",
    "        unique_titles = []\n",
    "        unique_categories = []\n",
    "        unique_tags = []\n",
    "\n",
    "        # Populate filter options from document metadata\n",
    "        for doc in st.session_state.documents:\n",
    "            title = doc.metadata.get('title')\n",
    "            if title is not None:\n",
    "                unique_titles.append(str(title))\n",
    "            categories = doc.metadata.get('categories')\n",
    "            if categories:\n",
    "                unique_categories.extend(categories)\n",
    "            tags = doc.metadata.get('tags')\n",
    "            if tags:\n",
    "                unique_tags.extend(tags)\n",
    "\n",
    "        # Get unique sorted lists for dropdowns/multiselects\n",
    "        unique_titles = sorted(list(set(unique_titles)))\n",
    "        unique_categories = sorted(list(set(unique_categories)))\n",
    "        unique_tags = sorted(list(set(unique_tags)))\n",
    "\n",
    "        # Display filter widgets\n",
    "        selected_title = st.selectbox(\"Select Title\", options=[None] + unique_titles)\n",
    "        selected_categories = st.multiselect(\"Select Categories\", options=unique_categories)\n",
    "        selected_tags = st.multiselect(\"Select Tags\", options=unique_tags)\n",
    "\n",
    "    # Preloaded Prompts (Buttons)\n",
    "    st.write(\"Quick Questions:\")\n",
    "    col1, col2, col3 = st.columns(3)\n",
    "\n",
    "    # Set user_query in session state if a button is clicked\n",
    "    if col1.button(\"Can you provide specific details about brewtiful?\", key=\"button1\"):\n",
    "        st.session_state.user_query = \"Can you provide specific details about brewtiful?\"\n",
    "    if col2.button(\"What is the current forecast for Enterprise clients?\", key=\"button2\"):\n",
    "        st.session_state.user_query = \"What is the current forecast for Enterprise clients?\"\n",
    "    if col3.button(\"Describe the ongoing activities related to the cloud.\", key=\"button3\"):\n",
    "        st.session_state.user_query = \"Describe the ongoing activities related to the cloud.\"\n",
    "\n",
    "    # --- Step 8. User Interaction (Chat Interface) ---\n",
    "    # Display existing messages\n",
    "    for message in st.session_state.messages:\n",
    "        with st.chat_message(message[\"role\"]):\n",
    "            st.markdown(message[\"content\"])\n",
    "\n",
    "    # Get new user input\n",
    "    user_query_input = st.chat_input(\"Ask a question...\")\n",
    "    if user_query_input:\n",
    "        st.session_state.user_query = user_query_input\n",
    "\n",
    "    # --- Query Handling (Steps 9-12) ---\n",
    "    if st.session_state.user_query:\n",
    "        # Display user query\n",
    "        with st.chat_message(\"user\"):\n",
    "            st.markdown(st.session_state.user_query)\n",
    "        # Add user query to message history for display (Part of Step 11)\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": st.session_state.user_query})\n",
    "\n",
    "    # --- Step 9. Dynamic Filtering and Retrieval Setup ---\n",
    "        filtered_docs = filter_documents(st.session_state.documents, selected_title, selected_categories, selected_tags)\n",
    "\n",
    "        if filtered_docs:\n",
    "            # Create a temporary filtered vector store and retriever if docs remain after filtering\n",
    "            db_filtered = FAISS.from_documents(filtered_docs, st.session_state.embeddings)\n",
    "            dynamic_retriever = db_filtered.as_retriever(search_kwargs={\"k\": 3})\n",
    "        else:\n",
    "            # If filtering removes all docs, create a dummy retriever and warn\n",
    "            dynamic_retriever = RunnableLambda(lambda x: []) # Returns an empty list\n",
    "            st.warning(\"No documents match the filters.\") # Warning displayed in main area\n",
    "\n",
    "    # --- Step 10. Chain Creation and Invocation ---\n",
    "        # Proceed only if documents were found after filtering\n",
    "        if filtered_docs:\n",
    "            chain = create_chain(st.session_state.llm, dynamic_retriever, st.session_state.memory)\n",
    "            try:\n",
    "                with st.spinner(\"Thinking...\"):\n",
    "                    # Get response object\n",
    "                    response_obj = chain.invoke(st.session_state.user_query)\n",
    "                # Extract the content from the response object\n",
    "                response_content = response_obj.content if hasattr(response_obj, 'content') else str(response_obj)\n",
    "\n",
    "    # --- Step 11. Response Handling (Success) ---\n",
    "                # Add interaction to memory\n",
    "                st.session_state.memory.chat_memory.add_user_message(st.session_state.user_query)\n",
    "                st.session_state.memory.chat_memory.add_ai_message(response_content)\n",
    "\n",
    "                # Display assistant response\n",
    "                with st.chat_message(\"assistant\"):\n",
    "                    st.markdown(response_content)\n",
    "                # Add assistant response to message history for display\n",
    "                st.session_state.messages.append({\"role\": \"assistant\", \"content\": response_content})\n",
    "\n",
    "    # --- Step 12. State Reset ---\n",
    "                # Clear button-pressed query state only if it wasn't from chat_input this run\n",
    "                if st.session_state.user_query != user_query_input:\n",
    "                    st.session_state.user_query = None\n",
    "\n",
    "            except Exception as e:\n",
    "                # Catch-all for errors during chain creation or invocation\n",
    "                st.error(f\"An error occurred: {e}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f990765",
   "metadata": {},
   "source": [
    "### 3.2 Run Application\n",
    "The `if __name__ == \"__main__\":` block ensures that the `main()` function is called only when the script is executed directly (not when imported as a module). This is the standard Python entry point for running the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ba6cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.2 Run Application ---\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notes_env .nosync",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
